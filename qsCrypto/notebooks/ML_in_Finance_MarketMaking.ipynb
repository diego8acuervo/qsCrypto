{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Market Making Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build on the previous two examples by considering the problem of high frequency market making. Unlike the previous example, we shall learn a time independent optimal policy.\n",
    "\n",
    "Assume that a market maker seeks to capture the bid-ask spread by placing one lot best bid and ask limit orders. They are required to strictly keep their inventory between -1 and 1. The problem is when to optimally quote either a bid or ask, or simply wait, each time there is a limit order book update. For example, sometimes it may be more advantageous to quote a bid to close out a short position if it will almost surely give an instantaneous net reward, other times it may be better to wait and capture a larger spread.\n",
    "\n",
    "In this toy example, the agent uses the liquidity imbalance in the top of the order book as a proxy for price movement and, hence, fill probabilities. The example does not use market orders, knowledge of queue positions, cancellations and limit order placement at different levels of the ladder. These are left to later material and exercises.\n",
    "\n",
    "At each non-uniform time update, $t$, the market feed provides best prices and depths $\\{p^a_t, q^a_t, p^b_t, q^b_t\\}$. The state space is the product of the inventory, $X_t\\in\\{-1,0,1\\}$, and gridded liquidity ratio $\\hat{R}_t= \\lfloor{\\frac{q^a_t}{q^a_t+q^b_t}N\\rfloor}\\in [0,1]$, where $N$ is the number of grid points and $q^a_t$ and $q^b_t$ are the depths of the best ask and bid. $\\hat{R}_t \\rightarrow 0$ is the regime where the mid-price will go up and an ask is filled. Vice versa for $\\hat{R}_t \\rightarrow 1$. The dimension of the state space is chosen to be $ 3 \\cdot 10 = 30$.\n",
    "\n",
    "A bid is filled with probability $\\epsilon_t:=\\hat{R}_t$ and an ask is filled with probability $1-\\epsilon_t$. The rewards are chosen to be the expected total P\\&L. If a bid is filled to close out a short holding, then the expected reward $r_t=-\\epsilon_t (\\Delta p_t+c)$, where $\\Delta p_t$ is the difference between the exit and entry price and $c$ is the transaction cost. For example, if the agent entered a short position at time $s<t$ with a filled ask at $p^a_s=100$ and closed out the position with a filled bid at $p^b_t=99$, then $\\Delta p_t=1$. The agent is penalized for quoting an ask or bid when the position is already short or long respectively.\n",
    "\n",
    "We can now apply SARSA or Q-learning to learn optimal market making in such a simplified setting. For exploration needed for on-line learning, one can use a\n",
    "$\\varepsilon $-greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up  \n",
    "#### Setting some global parameters  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of the reinforcement learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 0.5 # Probability for exploration\n",
    "\n",
    "ALPHA = 0.05 # Step size\n",
    "\n",
    "GAMMA = 1 # Discount factor for Q-Learning and Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters describing the problem and our implementation of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = [0, 1, 2] # Possible actions\n",
    "\n",
    "NUM_INV_STEPS = 3 # Number of inventory states: long, short, flat\n",
    "\n",
    "NUM_PROB_STEPS = 10 # Number of discrete probabilities\n",
    "\n",
    "# Dimensions of the action-state value array:\n",
    "Q_DIMS = (NUM_INV_STEPS, NUM_PROB_STEPS, len(ACTIONS))\n",
    "\n",
    "FILL_PROBS = np.linspace(0, 1, 10) # Possible probability values\n",
    "\n",
    "c = 0 # Transaction cost\n",
    "\n",
    "MAX_ITER = float('inf') # Maximum number of iterations in one episode\n",
    "# (with `MAX_ITER = np.float('inf')`, the entire dataset will be used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the discrete probability values in `FILL_PROBS`. These represent the probability of a bid being fulfilled, and the complement of the probability of an ask being fulfilled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILL_PROBS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These dictionaries map the names of the actions and positions to their index along the corresponding axis of the state-action value array `q_value` in the learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = {'buy': 2, 'sell': 0, 'hold': 1} \n",
    "positions = {'flat': 0, 'long': 2, 'short': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Fetch\n",
    "Fetch top of book historical data from some exchange using CoinApi historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CoinAPIPriceHandler.__init__() missing 1 required positional argument: 'events_queue'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Now import directly from the data module\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CoinAPIPriceHandler\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m ds = \u001b[43mCoinAPIPriceHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBTCUSDT\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m quotes = ds.get_historical_quotes(\u001b[33m\"\u001b[39m\u001b[33mBTCUSDT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1MIN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2026-02-19T00:00:00\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2026-02-20T23:59:59\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: CoinAPIPriceHandler.__init__() missing 1 required positional argument: 'events_queue'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the path to import local modules\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "if parent_dir not in sys.path:\n",
    "\tsys.path.insert(0, parent_dir)\n",
    "\n",
    "# Now import directly from the data module\n",
    "from data.price import CoinAPIPriceHandler\n",
    "\n",
    "\n",
    "\n",
    "ds = CoinAPIPriceHandler('BTCUSDT')\n",
    "\n",
    "quotes = ds.get_historical_quotes(\"BTCUSDT\", \"1MIN\", \"2026-02-19T00:00:00\", \"2026-02-20T23:59:59\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data generator\n",
    "\n",
    "The training data are in a .csv file. The data generator object yields the next Limit Order Book update from the file. When it reaches the end of the file, it raises `StopIteration`, and its `rewind()` method must be called to reset it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DataFeed(object):\n",
    "    def __init__(self, data_RA):\n",
    "        self.data_RA = data_RA\n",
    "        self.rewind()\n",
    "    def next(self):\n",
    "        try:\n",
    "            return self.__gen.__next__()\n",
    "        except StopIteration as e:\n",
    "            raise e\n",
    "    def rewind(self):\n",
    "        self.__gen = (row for row in self.data_RA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '../data/AMZN-L1.csv'\n",
    "\n",
    "data_RA = np.genfromtxt(csv_path, delimiter=',', dtype=float)\n",
    "data_generator = DataFeed(data_RA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State\n",
    "The state has four elements: \n",
    "- position (flat, long, short); \n",
    "- probability of ask fill (index in the array of probabilities)\n",
    "- prices (a dictionary of bid and ask)\n",
    "- entry price\n",
    "\n",
    "We note, however, that the q-value is only a function of the position and probability, and the action taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a function to \"rewind\" the data generator to the beginning of the dataset and initialise the state vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_state(data_generator):\n",
    "    data_generator.rewind()\n",
    "    \n",
    "    # By convention we start with a flat position\n",
    "    # and, therefore, no entry price\n",
    "    position = positions['flat']\n",
    "    entry_price = None\n",
    "    \n",
    "    ask, ask_depth, bid, bid_depth = data_generator.next()\n",
    "    \n",
    "    price = {'bid': bid/1000.0, 'ask': ask/1000.0}    \n",
    "    \n",
    "    # Estimate the fill probability\n",
    "    q = bid_depth / (bid_depth + ask_depth)\n",
    "    # Quantise q and scale it to the integer index \n",
    "    # q_ind is an index of the vector `FILL_PROBS`\n",
    "    q_ind = int(q * NUM_PROB_STEPS) \n",
    "    \n",
    "    initial_state = position, q_ind, price, entry_price\n",
    "    \n",
    "    return initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = get_initial_state(data_generator)\n",
    "print(START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step function that describes how the next state is obtained from the current state and the action taken. \n",
    "\n",
    "The function returns the next state and the immediate reward obtained from the action taken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "        position, q, price, entry_price = state\n",
    "        reward = 0 \n",
    "        instant_pnl = 0\n",
    "        done = False\n",
    "        \n",
    "        # The ask/bid fill probabilities always sum to 1, meaning\n",
    "        # that either bid or ask orders can always be executed \n",
    "        if FILL_PROBS[q] < np.random.rand():\n",
    "            fill_bid = True\n",
    "            fill_ask = False\n",
    "        else:\n",
    "            fill_bid = False\n",
    "            fill_ask = True\n",
    "        \n",
    "        # Calculate the result of taking the selected action\n",
    "        if (action == actions['buy']) and (fill_bid):\n",
    "            reward = -c\n",
    "            if (position == positions['flat']):         \n",
    "                position = positions['long']\n",
    "                entry_price = price['bid'] \n",
    "            elif(position == positions['short']): # closing out a short position          \n",
    "                position = positions['flat']\n",
    "                exit_price =  price['bid']\n",
    "                instant_pnl = entry_price - exit_price\n",
    "                entry_price = None\n",
    "            elif position == positions['long']:\n",
    "                raise ValueError(\"can't buy already got\")\n",
    "        \n",
    "        elif (action == actions['sell']) and (fill_ask):\n",
    "            reward = -c\n",
    "            if (position == positions['flat']):\n",
    "                position = positions['short']\n",
    "                entry_price = price['ask']\n",
    "            elif (position == positions['long']): # closing out a long position   \n",
    "                exit_price = price['ask']\n",
    "                position = positions['flat']\n",
    "                instant_pnl = exit_price - entry_price\n",
    "                entry_price = None\n",
    "            elif position == positions['short']:\n",
    "                raise ValueError(\"can't sell already short\")\n",
    "        \n",
    "        reward += instant_pnl\n",
    "        \n",
    "        try:            \n",
    "            # Get the next limit order book update\n",
    "            ask, ask_depth, bid, bid_depth = data_generator.next()\n",
    "            \n",
    "            # Calculate the price and bid/ask fill probabilities for the next state\n",
    "            price = {'bid': bid/1000.0, 'ask': ask/1000.0}    \n",
    "\n",
    "            # Estimate the fill probability\n",
    "            q = bid_depth / (bid_depth + ask_depth)\n",
    "            \n",
    "            # Quantise q and scale it to the integer index \n",
    "            # q_ind is an index of the vector `FILL_PROBS`\n",
    "            q_ind = int(q * NUM_PROB_STEPS) \n",
    "            \n",
    "        except StopIteration as e:\n",
    "            # This happens when the data generator reaches the end of the dataset\n",
    "            raise e\n",
    "        \n",
    "        next_state = position, q_ind, price, entry_price\n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check START state, action pairs and the associated reward\n",
    "print(actions)\n",
    "state = get_initial_state\n",
    "print(step(START, 0))\n",
    "print(step(START, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the agent's action policy\n",
    "Given $S_t$ and $Q_t\\left( s_t, a_t\\right)$, this function chooses an action based on the epsilon-greedy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, q_value, eps=EPSILON):\n",
    "    position, q, price, entry_price = state\n",
    "    \n",
    "    # With probability eps we choose randomly among allowed actions\n",
    "    if np.random.binomial(1, eps) == 1: \n",
    "        if position == positions['long']:\n",
    "            action = np.random.choice([actions['hold'], actions['sell']])\n",
    "        elif position == positions['short']:\n",
    "            action = np.random.choice([actions['hold'], actions['buy']])\n",
    "        else:\n",
    "            action = np.random.choice([actions['hold'], actions['buy'], actions['sell']])  \n",
    "        \n",
    "    # Otherwise the best available action is selected\n",
    "    else:\n",
    "        # Make a list of the actions available from the current state\n",
    "        if position == positions['long']:\n",
    "            actions_ = [actions['hold'], actions['sell']]        \n",
    "        elif position == positions['short']:\n",
    "            actions_ = [actions['hold'], actions['buy']]\n",
    "        else:\n",
    "            actions_ = [actions['hold'], actions['buy'], actions['sell']]\n",
    "        # Get the state-action values for the current state\n",
    "        values_ = q_value[state[0], state[1], actions_]\n",
    "        # In case of a tie, choose from those with the highest value\n",
    "        action = np.random.choice([actions_[action_] for action_, value_ in enumerate(values_) \n",
    "                                 if value_ == np.max(values_)])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random state-action value function\n",
    "q_value_example = np.random.random(Q_DIMS) \n",
    "\n",
    "# Show the initial state\n",
    "state = get_initial_state(data_generator)\n",
    "print(state)\n",
    "\n",
    "# The action values for the initial state. \n",
    "# state[0] is the position; state[1] is the bid fill probability\n",
    "print(q_value_example[state[0], state[1], :])\n",
    "\n",
    "# With epsilon = 0, the selected action is always that with the highest Q-value\n",
    "print(choose_action(state, q_value_example, eps=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sarsa and Expected Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below runs through a learning episode with Sarsa. It takes the state-action value array `q_value` as an argument, initialises the state to `START`, defined above, and updates `q_value` according to the Sarsa algorithm, until reaching either the end of the training data or the maximum number of iterations. The cumulative reward earned is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(q_value, expected=False, step_size=ALPHA, eps=EPSILON):\n",
    "    \n",
    "    state = get_initial_state(data_generator)\n",
    "    \n",
    "    action = choose_action(state, q_value, eps)\n",
    "    rewards = 0.0\n",
    "    done = False\n",
    "    iteration = 0\n",
    "    \n",
    "    while (iteration < MAX_ITER) and not done:\n",
    "        # The step function will raise StopIteration when there\n",
    "        # is no more data available to calculate the next state:\n",
    "        try:\n",
    "            next_state, reward = step(state, action)\n",
    "        except StopIteration:\n",
    "            # Skip the rest of the loop and end the episode.\n",
    "            # As there is no new `next_state`, updating\n",
    "            # q_value again doesn't make sense\n",
    "            done = True\n",
    "            continue\n",
    "        next_action = choose_action(next_state, q_value, eps)\n",
    "        \n",
    "        rewards += reward\n",
    "        \n",
    "        if not expected:\n",
    "            target = q_value[next_state[0], next_state[1], next_action]\n",
    "        else:\n",
    "            # Calculate the expected value of new state for expected SARSA\n",
    "            target = 0.0\n",
    "            q_next = q_value[next_state[0], next_state[1], :]\n",
    "            best_actions = np.argwhere(q_next == np.max(q_next))\n",
    "            for action_ in ACTIONS: \n",
    "                if action_ in best_actions:\n",
    "                    target += ((1.0 -  eps) / len(best_actions) \n",
    "                               +  eps / len(ACTIONS)) * q_value[next_state[0], next_state[1], action_]\n",
    "                else:\n",
    "                    target +=  eps / len(ACTIONS) * q_value[next_state[0], next_state[1], action_]\n",
    "        target *= GAMMA\n",
    "        \n",
    "        # SARSA update\n",
    "        q_value[state[0], state[1], action] += step_size * (reward\n",
    "                 + target - q_value[state[0], state[1], action])\n",
    "        \n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        iteration += 1\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function simulates an episode with Q-learning. It takes the state-action value array `q_value` as an argument, initialises the state to `START`, defined above, and updates `q_value` according to the Q-learning algorithm, until the $T$ time steps have passed, or the stocks have all been sold. The cumulative reward earned is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(q_value, step_size=ALPHA, eps=EPSILON):\n",
    "    \n",
    "    state = get_initial_state(data_generator)\n",
    "    \n",
    "    rewards = 0.0\n",
    "    done  = False\n",
    "    iteration = 0\n",
    "    \n",
    "    while (iteration < MAX_ITER) and not done:\n",
    "        action = choose_action(state, q_value, eps)\n",
    "        # The step function will raise StopIteration when there\n",
    "        # is no more data available to calculate the next state:\n",
    "        try:\n",
    "            next_state, reward = step(state, action)\n",
    "        except StopIteration:\n",
    "            # Skip the rest of the loop and end the episode.\n",
    "            # As there is no new `next_state`, updating\n",
    "            # q_value again doesn't make sense\n",
    "            done = True\n",
    "            continue\n",
    "        \n",
    "        rewards += reward\n",
    "        \n",
    "        # Q-Learning update\n",
    "        q_value[state[0], state[1], action] += step_size * (\n",
    "                reward + GAMMA * np.max(q_value[next_state[0], next_state[1], :]) -\n",
    "                q_value[state[0], state[1], action])\n",
    "        state = next_state\n",
    "        iteration +=1\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing the learned policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will allow us to inspect the optimal action learned for each of the possible states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_optimal_policy(q_value):\n",
    "    \n",
    "    optimal_policy = np.argmax(q_value, axis=-1)\n",
    "    print(\"ask fill prob:\", *['%.2f' % q for q in  FILL_PROBS])\n",
    "    \n",
    "    for i in range(0, NUM_INV_STEPS):\n",
    "        \n",
    "        # positions ={'flat': 0, 'long': 2, 'short':1}\n",
    "        str_=\"\"\n",
    "        if (i==0):\n",
    "            str_ += '         flat     '\n",
    "        elif(i==1):\n",
    "            str_ += '        short     '\n",
    "        else:\n",
    "            str_ += '         long     '\n",
    "            \n",
    "        for j in range(0, NUM_PROB_STEPS): \n",
    "            a = np.int(optimal_policy[i,j])\n",
    "            # actions = {'buy':2, 'sell':0, 'hold': 1}\n",
    "            if a == 0:\n",
    "                str_ += 's    '\n",
    "            elif a ==1:\n",
    "                str_ += 'h    ' \n",
    "            else:\n",
    "                str_ += 'b    '  \n",
    "        print(str_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the epsilon decay\n",
    "\n",
    "We decrease the value of epsilon with each epoch - epsilon must approach zero as the number of episodes increases in order to ensure that the q-value function converges to the optimum\n",
    "\n",
    "The following figure demonstrates the exponential decay we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "eps = 0.5\n",
    "epis = 150\n",
    "epoch = 15\n",
    "\n",
    "plt.plot([eps*((1-eps)**(i//epoch)) for i in range(epis)])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Sarsa and Q-learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, episodes=150):\n",
    "    if policy == 'sarsa':\n",
    "        learning_alg = sarsa\n",
    "    elif policy == 'q-learning':\n",
    "        learning_alg = q_learning\n",
    "    else:\n",
    "        raise ValueError(\"choose 'sarsa' or 'q-learning'\")\n",
    "    \n",
    "    epoch_length = 15\n",
    "    \n",
    "    # Initialise the rewards vector and state-action values array\n",
    "    rewards = np.zeros(episodes)\n",
    "    q_value = np.zeros(Q_DIMS)\n",
    "    \n",
    "    print('Training {}...'.format(policy))\n",
    "    for i in tqdm(range(0, episodes)):\n",
    "        eps = EPSILON*((1-EPSILON)**(i//epoch_length))\n",
    "        rewards[i] = learning_alg(q_value, eps=eps)\n",
    "    \n",
    "    return q_value, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "q_sarsa, rewards_sarsa = train('sarsa')\n",
    "q_q_learning, rewards_q_learning = train('q-learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SARSA')\n",
    "print_optimal_policy(q_sarsa)\n",
    "print('Q-learning')\n",
    "print_optimal_policy(q_q_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(13,8))\n",
    "plt.plot(rewards_q_learning, label='Q-Learning')\n",
    "plt.plot(rewards_sarsa, label='SARSA')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Sum of rewards during episode')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation of the resulting market making strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below will run through the dataset, taking actions according to the state-action values learned during the training process above. \n",
    "\n",
    "You can choose to use either of the strategies learned by SARSA and Q-learning by assigning them to the `view_strategy` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_strategy = q_q_learning\n",
    "#view_strategy = q_sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "%matplotlib nbagg\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "gs = GridSpec(2,2) # 2 rows, 2 columns\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0]) # First row, first column\n",
    "ax2 = fig.add_subplot(gs[0,1]) # First row, second column\n",
    "ax3 = fig.add_subplot(gs[1,0]) # Second row, first column\n",
    "\n",
    "bids = []\n",
    "asks = []\n",
    "bid_fills = []\n",
    "xdata = []\n",
    "pnl = []\n",
    "\n",
    "done = False\n",
    "state = get_initial_state(data_generator)\n",
    "rewards = 0.0\n",
    "iteration = 0\n",
    "\n",
    "while iteration < MAX_ITER and not done:\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        prev_position_name = [name for name, pos in positions.items() if pos == state[0]][0] \n",
    "        \n",
    "        action = np.argmax(view_strategy[state[0], state[1], :])\n",
    "        try:\n",
    "            state, reward = step(state, action)\n",
    "        except StopIteration:\n",
    "            done = True\n",
    "            print('Stopped at time step', iteration)\n",
    "            continue\n",
    "        iteration += 1\n",
    "        \n",
    "        position_name = [name for name, pos in positions.items() if pos == state[0]][0] \n",
    "        action_name = [name for name, act in actions.items() if act == action][0]                    \n",
    "        prices = state[2]\n",
    "        if state[3] is None:\n",
    "            entry_price = 'n/a'\n",
    "        else:\n",
    "            entry_price = \"%.2f\" % state[3]\n",
    "        \n",
    "        # Cumulative PnL\n",
    "        if len(pnl) == 0:\n",
    "            pnl.append(reward)\n",
    "        else: \n",
    "            pnl.append(pnl[-1]+reward)\n",
    "\n",
    "        bids.append(prices['bid'])\n",
    "        asks.append(prices['ask'])\n",
    "        xdata.append(iteration)\n",
    "        \n",
    "        # Plot most recent 80 prices\n",
    "        ax1.plot(xdata, \n",
    "                 bids, color = 'black')\n",
    "        ax1.plot(xdata, \n",
    "                 asks, color = 'black')\n",
    "        ax1.set_ylabel('Prices')\n",
    "        ax1.set_xlabel('Iteration')\n",
    "        ax1.set_title('Cumulated PnL: ' + \"%.2f\" % pnl[-1] + ' ~ '\n",
    "                     + 'Position: ' + position_name + ' ~ '\n",
    "                     + 'Entry Price: ' + entry_price)\n",
    "        ax1.set_xlim([max(0, iteration - 80.5), iteration + 0.5])\n",
    "\n",
    "        # Plotting actions taken according to the Policy\n",
    "        if position_name != prev_position_name:\n",
    "            if action == actions['sell']:\n",
    "                ax1.scatter(iteration, prices['bid']+0.1, \n",
    "                        color='orangered', marker='v', s=50)\n",
    "            elif action == actions['buy']:\n",
    "                ax1.scatter(iteration, prices['ask']-0.1, \n",
    "                        color='lawngreen', marker='^', s=50)\n",
    "        \n",
    "        # Ploting PnL\n",
    "        ax2.clear()\n",
    "        ax2.plot(xdata, pnl)\n",
    "        ax2.set_ylabel('Total PnL')\n",
    "        ax2.set_xlabel('Iteration')\n",
    "\n",
    "        # Plotting current probabilities to fill\n",
    "        q_a = FILL_PROBS[state[1]]\n",
    "        q_b = 1 - q_a\n",
    "        performance = [q_b, q_a]\n",
    "\n",
    "        ax3.clear()\n",
    "        ax3.bar([0, 1], [q_b, q_a], align='center', alpha=0.5, \n",
    "                color=['orangered','lawngreen'])\n",
    "        ax3.set_xticks([0, 1])\n",
    "        ax3.set_xticklabels(['bid', 'ask'])\n",
    "        ax3.set_title('Probability of fill')\n",
    "        ax3.set_ylim([0, 1])\n",
    "        fig.tight_layout()\n",
    "        fig.canvas.draw()\n",
    "        time.sleep(max(0, 0.5 - (time.time() - start_time)))\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print('Animation stopped')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
